import requests
from datetime import datetime, timedelta
from .base_agent import BaseAgent

class TechNewsAgent(BaseAgent):
    """ITç§‘æŠ€æ–°é—»æ™ºèƒ½ä½“"""
    
    def __init__(self):
        """åˆå§‹åŒ–ITç§‘æŠ€æ–°é—»æ™ºèƒ½ä½“"""
        super().__init__()
        self.categories = ["technology", "science"]
        self.en_keywords = [
            # äººå·¥æ™ºèƒ½ç›¸å…³
            "AI", "artificial intelligence", "machine learning", "deep learning", "neural networks",
            "language models", "LLM", "GPT", "natural language processing", "NLP",
            "computer vision", "robotics", "autonomous systems", "algorithm",
            
            # åŸºç¡€æŠ€æœ¯
            "tech", "technology", "software", "hardware", "cloud computing", 
            "big data", "data science", "database", "programming", "code",
            
            # æ–°å…´æŠ€æœ¯é¢†åŸŸ
            "digital transformation", "IoT", "Internet of Things", "blockchain", "cryptocurrency",
            "bitcoin", "ethereum", "web3", "metaverse", "augmented reality", "AR",
            "virtual reality", "VR", "quantum computing", "edge computing",
            
            # ç§»åŠ¨ä¸æ¶ˆè´¹ç”µå­
            "smartphone", "mobile technology", "wearable tech", "smartwatch", "gadget",
            "consumer electronics", "laptop", "5G", "6G", "wireless technology",
            
            # ä¼ä¸šä¸äº§ä¸š
            "tech industry", "startup", "innovation", "digital", "enterprise software", 
            "SaaS", "fintech", "healthtech", "edtech", "tech company",
            "tech investment", "venture capital", "tech regulation", "tech policy"
        ]
        
        self.zh_keywords = [
            # äººå·¥æ™ºèƒ½ç›¸å…³
            "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "å¤§æ¨¡å‹", "å¤§è¯­è¨€æ¨¡å‹",
            "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "æœºå™¨äºº", "è‡ªåŠ¨é©¾é©¶", "ç®—æ³•", "æ™ºèƒ½åŠ©æ‰‹",
            
            # åŸºç¡€æŠ€æœ¯
            "ç§‘æŠ€", "æŠ€æœ¯", "è½¯ä»¶", "ç¡¬ä»¶", "äº‘è®¡ç®—", "å¤§æ•°æ®", "æ•°æ®ç§‘å­¦", 
            "æ•°æ®åº“", "ç¼–ç¨‹", "ä»£ç ", "å¼€å‘", "ç³»ç»Ÿæ¶æ„",
            
            # æ–°å…´æŠ€æœ¯é¢†åŸŸ
            "æ•°å­—åŒ–", "æ•°å­—åŒ–è½¬å‹", "ç‰©è”ç½‘", "åŒºå—é“¾", "åŠ å¯†è´§å¸", "æ¯”ç‰¹å¸", 
            "ä»¥å¤ªåŠ", "Web3", "å…ƒå®‡å®™", "å¢å¼ºç°å®", "è™šæ‹Ÿç°å®", "é‡å­è®¡ç®—", "è¾¹ç¼˜è®¡ç®—",
            
            # ç§»åŠ¨ä¸æ¶ˆè´¹ç”µå­
            "æ™ºèƒ½æ‰‹æœº", "ç§»åŠ¨æŠ€æœ¯", "å¯ç©¿æˆ´è®¾å¤‡", "æ™ºèƒ½æ‰‹è¡¨", "ç”µå­è®¾å¤‡", "æ¶ˆè´¹ç”µå­",
            "ç¬”è®°æœ¬ç”µè„‘", "5G", "6G", "æ— çº¿æŠ€æœ¯", "æ™ºèƒ½å®¶å±…",
            
            # ä¼ä¸šä¸äº§ä¸š
            "ç§‘æŠ€äº§ä¸š", "åˆ›æ–°", "äº’è”ç½‘", "åˆ›ä¸šå…¬å¸", "ä¼ä¸šè½¯ä»¶", "è½¯ä»¶å³æœåŠ¡", 
            "é‡‘èç§‘æŠ€", "å¥åº·ç§‘æŠ€", "æ•™è‚²ç§‘æŠ€", "ç§‘æŠ€å…¬å¸", "ç§‘æŠ€æŠ•èµ„", 
            "é£é™©æŠ•èµ„", "ç§‘æŠ€ç›‘ç®¡", "ç§‘æŠ€æ”¿ç­–", "æ•°å­—ç»æµ"
        ]
    
    def collect_news(self, max_articles=5):
        """æ”¶é›†ITç§‘æŠ€ç›¸å…³æ–°é—»ï¼ŒåŒ…æ‹¬ä¸­è‹±æ–‡å„5æ¡
        
        Args:
            max_articles (int): æ¯ç§è¯­è¨€çš„æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± æ”¶é›†ITç§‘æŠ€æ–°é—»...")
        
        # è·å–å½“å‰æ—¥æœŸå’Œå‰ä¸€å¤©çš„æ—¥æœŸ
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        from_date = yesterday.strftime('%Y-%m-%d')
        
        # è·å–è‹±æ–‡æ–°é—»
        english_news = self._collect_language_news(self.en_keywords, "en", max_articles)
        
        # è·å–ä¸­æ–‡æ–°é—» - å¤šè·å–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        zh_max = max_articles * 2  # è·å–æ›´å¤šä¸­æ–‡æ–°é—»ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        chinese_news = self._collect_language_news(self.zh_keywords, "zh", zh_max)
        
        # è¿‡æ»¤ç¹ä½“ä¸­æ–‡æ–°é—»
        simplified_chinese_news = []
        filtered_count = 0
        
        for news in chinese_news:
            # æ£€æŸ¥æ ‡é¢˜å’Œæè¿°æ˜¯å¦åŒ…å«ç¹ä½“ä¸­æ–‡
            title = news.get('title', '')
            desc = news.get('description', '')
            
            if self.is_traditional_chinese(title) or self.is_traditional_chinese(desc):
                filtered_count += 1
                continue
            
            simplified_chinese_news.append(news)
            
            # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿæ•°é‡çš„ç®€ä½“ä¸­æ–‡æ–°é—»ï¼Œå°±åœæ­¢
            if len(simplified_chinese_news) >= max_articles:
                break
        
        if filtered_count > 0:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± å·²è¿‡æ»¤ {filtered_count} æ¡ç¹ä½“ä¸­æ–‡æ–°é—»")
        
        # ç¡®ä¿ä¸è¶…è¿‡max_articlesæ•°é‡
        simplified_chinese_news = simplified_chinese_news[:max_articles]
        
        # åˆå¹¶ç»“æœ
        combined_news = english_news + simplified_chinese_news
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± æ”¶é›†å®Œæˆ: {len(combined_news)} æ¡ (è‹±:{len(english_news)}, ä¸­:{len(simplified_chinese_news)})")
        return combined_news
    
    def _collect_language_news(self, keywords, language, max_articles):
        """æ”¶é›†ç‰¹å®šè¯­è¨€çš„æ–°é—»
        
        Args:
            keywords (list): æœç´¢å…³é”®è¯åˆ—è¡¨
            language (str): è¯­è¨€ä»£ç ï¼Œ'en'ä¸ºè‹±æ–‡ï¼Œ'zh'ä¸ºä¸­æ–‡
            max_articles (int): æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: è¯¥è¯­è¨€çš„æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        lang_label = "è‹±æ–‡" if language == "en" else "ä¸­æ–‡"
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± è·å–{lang_label}ç§‘æŠ€æ–°é—»...")
        
        # å¯¼å…¥éšæœºæ¨¡å—
        import random
        
        # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        # åªé€‰å–å‰30ä¸ªå…³é”®è¯ï¼Œé¿å…è¿‡å¤šæŸ¥è¯¢
        selected_keywords = shuffled_keywords[:30]
        
        # å°†å…³é”®è¯åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š10ä¸ªå…³é”®è¯
        batch_size = 10
        keywords_batches = [selected_keywords[i:i + batch_size] for i in range(0, len(selected_keywords), batch_size)]
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± ä»{len(keywords)}ä¸ªå…³é”®è¯ä¸­éšæœºé€‰æ‹©{len(selected_keywords)}ä¸ªï¼Œåˆ†ä¸º{len(keywords_batches)}æ‰¹è¿›è¡ŒæŸ¥è¯¢")
        
        # æ„å»ºNewsAPIè¯·æ±‚URL
        base_url = "https://newsapi.org/v2/everything"
        
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡è·å–çš„æ–‡ç« 
        all_articles = []
        
        # è®¾ç½®æå‰ç»ˆæ­¢æ¡ä»¶ï¼šè·å–åˆ°12ç¯‡æ–‡ç« å°±åœæ­¢
        early_stop_count = 12
        
        # æŒ‰æ‰¹æ¬¡è·å–æ–‡ç« 
        for i, batch_keywords in enumerate(keywords_batches):
            # æ„å»ºæŸ¥è¯¢å…³é”®è¯
            query = " OR ".join(batch_keywords)
            
            # è®¾ç½®è¯·æ±‚å‚æ•°
            params = {
                "apiKey": self.news_api_key,
                "q": query,
                "from": (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                "language": language,
                "sortBy": "relevancy",
                "pageSize": 4  # æ¯æ‰¹æ¬¡è·å–å›ºå®šæ•°é‡çš„æ–‡ç« 
            }
            
            try:
                # å‘é€è¯·æ±‚
                response = requests.get(base_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                # è·å–æ–‡ç« åˆ—è¡¨
                batch_articles = data.get("articles", [])
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± æ‰¹æ¬¡ {i+1}/{len(keywords_batches)}: è·å–åˆ° {len(batch_articles)} ç¯‡æ–‡ç« ")
                
                # å°†è¯¥æ‰¹æ¬¡çš„æ–‡ç« æ·»åŠ åˆ°æ€»æ–‡ç« åˆ—è¡¨ä¸­
                all_articles.extend(batch_articles)
                
                # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œå¯ä»¥æå‰é€€å‡º
                if len(all_articles) >= early_stop_count:
                    print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± å·²è·å–è¶³å¤Ÿå¤šçš„æ–‡ç«  ({len(all_articles)} ç¯‡)ï¼Œåœæ­¢æŸ¥è¯¢")
                    break
                    
            except Exception as e:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è·å–{lang_label}ç§‘æŠ€æ–°é—»æ‰¹æ¬¡ {i+1} å¤±è´¥: {e}")
        
        # å»é™¤å¯èƒ½çš„é‡å¤æ–‡ç« ï¼ˆåŸºäºURLï¼‰
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            url = article.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± å»é‡åå…± {len(unique_articles)} ç¯‡{lang_label}æ–‡ç« ")
        
        # ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        if unique_articles:
            return self._filter_relevant_articles(unique_articles, max_articles, lang_label)
        else:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± æœªè·å–åˆ°{lang_label}æ–‡ç« ")
            return []
    
    def _filter_relevant_articles(self, articles, max_articles, language_label=""):
        """ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            max_articles (int): æœ€å¤§è¿”å›æ–‡ç« æ•°é‡
            language_label (str): è¯­è¨€æ ‡ç­¾ï¼Œç”¨äºæ—¥å¿—
            
        Returns:
            list: ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
        """
        # å¦‚æœæ–‡ç« æ•°é‡å°‘äºmax_articlesï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ–‡ç« 
        if len(articles) <= max_articles:
            return self._format_articles(articles, language_label)
        
        # æ„å»ºæç¤ºè¯
        article_summaries = []
        for i, article in enumerate(articles[:10]):  # åªå¤„ç†å‰10ç¯‡æ–‡ç« 
            title = article.get("title", "æ— æ ‡é¢˜")
            description = article.get("description", "æ— æè¿°")
            article_summaries.append(f"{i+1}. æ ‡é¢˜: {title}\n   æè¿°: {description}")
        
        article_text = "\n\n".join(article_summaries)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ITç§‘æŠ€æ–°é—»ç¼–è¾‘ï¼Œè¯·ä»ä»¥ä¸‹{language_label}ITç§‘æŠ€æ–°é—»ä¸­é€‰æ‹©{max_articles}ç¯‡æœ€é‡è¦ã€æœ€æœ‰å½±å“åŠ›çš„æ–‡ç« ï¼Œå®ƒä»¬åº”è¯¥æ¶µç›–é‡è¦æŠ€æœ¯çªç ´ã€äº§ä¸šåŠ¨æ€æˆ–å…·æœ‰å¹¿æ³›å½±å“çš„ITæ–°é—»ã€‚
        
æ–°é—»åˆ—è¡¨:
{article_text}

è¯·åªè¿”å›ä½ é€‰æ‹©çš„æ–‡ç« åºå·ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„æ•°å­—ï¼Œä¾‹å¦‚: 1,3,5
"""
        
        # è°ƒç”¨LLM API
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“± ç­›é€‰{language_label}æ–‡ç« ä¸­...")
        response = self.call_llm_api(prompt, temperature=0.3)
        
        try:
            # è§£æå“åº”ï¼Œè·å–é€‰ä¸­çš„æ–‡ç« ç´¢å¼•
            selected_indices = []
            for item in response.replace(" ", "").split(","):
                if item.isdigit() and 1 <= int(item) <= len(articles):
                    selected_indices.append(int(item) - 1)
            
            # æˆªå–è‡³max_articles
            selected_indices = selected_indices[:max_articles]
            
            # è¿”å›é€‰ä¸­çš„æ–‡ç« 
            selected_articles = [articles[i] for i in selected_indices]
            
            return self._format_articles(selected_articles, language_label)
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è§£æLLMå“åº”å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›å‰max_articlesç¯‡æ–‡ç« 
            return self._format_articles(articles[:max_articles], language_label)
    
    def _format_articles(self, articles, language_label=""):
        """æ ¼å¼åŒ–æ–‡ç« ä¸ºç»Ÿä¸€è¾“å‡ºæ ¼å¼
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            language_label (str): è¯­è¨€æ ‡ç­¾
            
        Returns:
            list: æ ¼å¼åŒ–åçš„æ–‡ç« åˆ—è¡¨
        """
        formatted_articles = []
        for article in articles:
            formatted_article = {
                "title": article.get("title", "æ— æ ‡é¢˜"),
                "description": article.get("description", "æ— æè¿°"),
                "url": article.get("url", ""),
                "source": article.get("source", {}).get("name", "æœªçŸ¥æ¥æº"),
                "published_at": article.get("publishedAt", ""),
                "category": "ITç§‘æŠ€",
                "language": language_label
            }
            formatted_articles.append(formatted_article)
        
        return formatted_articles 