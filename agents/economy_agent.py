import requests
from datetime import datetime, timedelta
from .base_agent import BaseAgent

class EconomyNewsAgent(BaseAgent):
    """ç»æµæ–°é—»æ™ºèƒ½ä½“"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»æµæ–°é—»æ™ºèƒ½ä½“"""
        super().__init__()
        self.categories = ["business"]
        self.en_keywords = [
            # å®è§‚ç»æµ
            "economy", "economic", "GDP", "inflation", "recession", "economic growth", 
            "monetary policy", "fiscal policy", "interest rate", "central bank", 
            "economic forecast", "economic outlook", "economic indicator",
            
            # é‡‘èå¸‚åœº
            "finance", "financial", "market", "stock market", "bond market", 
            "stock exchange", "securities", "equity", "market volatility", 
            "bull market", "bear market", "trading", "derivatives",
            
            # æŠ•èµ„ç›¸å…³
            "investment", "investor", "portfolio", "asset management", "wealth management",
            "hedge fund", "private equity", "venture capital", "mutual fund", "ETF",
            "dividend", "capital gains", "investment strategy", "return on investment",
            
            # ä¼ä¸šè´¢åŠ¡ä¸è¿è¥
            "business", "corporate", "earnings", "revenue", "profit", "loss", 
            "quarterly report", "balance sheet", "income statement", "cash flow",
            "merger", "acquisition", "IPO", "initial public offering", "bankruptcy",
            
            # å›½é™…è´¸æ˜“ä¸å…³ç³»
            "trade", "tariff", "trade deficit", "trade surplus", "trade war", 
            "global economy", "economic sanctions", "trade agreement", "export", "import",
            "economic cooperation", "economic integration", "globalization",
            
            # è¡Œä¸šä¸è¶‹åŠ¿
            "industry", "sector", "retail", "manufacturing", "technology sector", 
            "energy market", "real estate market", "commodities", "oil price", 
            "gold price", "supply chain", "labor market", "employment", "unemployment"
        ]
        
        self.zh_keywords = [
            # å®è§‚ç»æµ
            "ç»æµ", "å®è§‚ç»æµ", "å›½æ°‘ç”Ÿäº§æ€»å€¼", "GDP", "é€šè´§è†¨èƒ€", "é€šèƒ€", "ç»æµè¡°é€€", 
            "ç»æµå¢é•¿", "è´§å¸æ”¿ç­–", "è´¢æ”¿æ”¿ç­–", "åˆ©ç‡", "å¤®è¡Œ", "ä¸­å¤®é“¶è¡Œ", 
            "ç»æµé¢„æµ‹", "ç»æµå±•æœ›", "ç»æµæŒ‡æ ‡", "ç»æµæ•°æ®",
            
            # é‡‘èå¸‚åœº
            "é‡‘è", "é‡‘èå¸‚åœº", "è‚¡å¸‚", "å€ºåˆ¸å¸‚åœº", "è¯åˆ¸äº¤æ˜“æ‰€", "è¯åˆ¸", "è‚¡æƒ", 
            "å¸‚åœºæ³¢åŠ¨", "ç‰›å¸‚", "ç†Šå¸‚", "äº¤æ˜“", "è¡ç”Ÿå“", "æœŸè´§", "æœŸæƒ",
            
            # æŠ•èµ„ç›¸å…³
            "æŠ•èµ„", "æŠ•èµ„è€…", "æŠ•èµ„ç»„åˆ", "èµ„äº§ç®¡ç†", "è´¢å¯Œç®¡ç†", "å¯¹å†²åŸºé‡‘", 
            "ç§å‹Ÿè‚¡æƒ", "é£é™©æŠ•èµ„", "å…±åŒåŸºé‡‘", "äº¤æ˜“æ‰€äº¤æ˜“åŸºé‡‘", "åˆ†çº¢", 
            "èµ„æœ¬æ”¶ç›Š", "æŠ•èµ„ç­–ç•¥", "æŠ•èµ„å›æŠ¥", "ç†è´¢",
            
            # ä¼ä¸šè´¢åŠ¡ä¸è¿è¥
            "å•†ä¸š", "ä¼ä¸š", "ç›ˆåˆ©", "è¥æ”¶", "åˆ©æ¶¦", "äºæŸ", "å­£åº¦æŠ¥å‘Š", "èµ„äº§è´Ÿå€ºè¡¨", 
            "åˆ©æ¶¦è¡¨", "ç°é‡‘æµ", "å¹¶è´­", "é¦–æ¬¡å…¬å¼€å‹Ÿè‚¡", "IPO", "ç ´äº§", "å…¬å¸è´¢æŠ¥",
            
            # å›½é™…è´¸æ˜“ä¸å…³ç³»
            "è´¸æ˜“", "å…³ç¨", "è´¸æ˜“é€†å·®", "è´¸æ˜“é¡ºå·®", "è´¸æ˜“æˆ˜", "å…¨çƒç»æµ", "ç»æµåˆ¶è£", 
            "è´¸æ˜“åè®®", "å‡ºå£", "è¿›å£", "ç»æµåˆä½œ", "ç»æµä¸€ä½“åŒ–", "å…¨çƒåŒ–",
            
            # è¡Œä¸šä¸è¶‹åŠ¿
            "äº§ä¸š", "è¡Œä¸š", "é›¶å”®ä¸š", "åˆ¶é€ ä¸š", "ç§‘æŠ€è¡Œä¸š", "èƒ½æºå¸‚åœº", "æˆ¿åœ°äº§å¸‚åœº", 
            "å¤§å®—å•†å“", "æ²¹ä»·", "é‡‘ä»·", "ä¾›åº”é“¾", "åŠ³åŠ¨åŠ›å¸‚åœº", "å°±ä¸š", "å¤±ä¸š", 
            "æ•°å­—ç»æµ", "å…±äº«ç»æµ", "å¹³å°ç»æµ", "ç»æµè½¬å‹"
        ]
    
    def collect_news(self, max_articles=5):
        """æ”¶é›†ç»æµç›¸å…³æ–°é—»ï¼ŒåŒ…æ‹¬ä¸­è‹±æ–‡å„5æ¡
        
        Args:
            max_articles (int): æ¯ç§è¯­è¨€çš„æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° æ”¶é›†ç»æµæ–°é—»...")
        
        # è·å–å½“å‰æ—¥æœŸå’Œå‰ä¸€å¤©çš„æ—¥æœŸ
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        from_date = yesterday.strftime('%Y-%m-%d')
        
        # è·å–è‹±æ–‡æ–°é—»
        english_news = self._collect_language_news(self.en_keywords, "en", max_articles)
        
        # è·å–ä¸­æ–‡æ–°é—» - å¤šè·å–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        zh_max = max_articles * 2  # è·å–æ›´å¤šä¸­æ–‡æ–°é—»ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        chinese_news = self._collect_language_news(self.zh_keywords, "zh", zh_max)
        
        # è¿‡æ»¤ç¹ä½“ä¸­æ–‡æ–°é—»
        simplified_chinese_news = []
        filtered_count = 0
        
        for news in chinese_news:
            # æ£€æŸ¥æ ‡é¢˜å’Œæè¿°æ˜¯å¦åŒ…å«ç¹ä½“ä¸­æ–‡
            title = news.get('title', '')
            desc = news.get('description', '')
            
            if self.is_traditional_chinese(title) or self.is_traditional_chinese(desc):
                filtered_count += 1
                continue
            
            simplified_chinese_news.append(news)
            
            # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿæ•°é‡çš„ç®€ä½“ä¸­æ–‡æ–°é—»ï¼Œå°±åœæ­¢
            if len(simplified_chinese_news) >= max_articles:
                break
        
        if filtered_count > 0:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° å·²è¿‡æ»¤ {filtered_count} æ¡ç¹ä½“ä¸­æ–‡æ–°é—»")
        
        # ç¡®ä¿ä¸è¶…è¿‡max_articlesæ•°é‡
        simplified_chinese_news = simplified_chinese_news[:max_articles]
        
        # åˆå¹¶ç»“æœ
        combined_news = english_news + simplified_chinese_news
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° æ”¶é›†å®Œæˆ: {len(combined_news)} æ¡ (è‹±:{len(english_news)}, ä¸­:{len(simplified_chinese_news)})")
        return combined_news
    
    def _collect_language_news(self, keywords, language, max_articles):
        """æ”¶é›†ç‰¹å®šè¯­è¨€çš„æ–°é—»
        
        Args:
            keywords (list): æœç´¢å…³é”®è¯åˆ—è¡¨
            language (str): è¯­è¨€ä»£ç ï¼Œ'en'ä¸ºè‹±æ–‡ï¼Œ'zh'ä¸ºä¸­æ–‡
            max_articles (int): æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: è¯¥è¯­è¨€çš„æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        lang_label = "è‹±æ–‡" if language == "en" else "ä¸­æ–‡"
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° è·å–{lang_label}ç»æµæ–°é—»...")
        
        # å¯¼å…¥éšæœºæ¨¡å—
        import random
        
        # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        # åªé€‰å–å‰30ä¸ªå…³é”®è¯ï¼Œé¿å…è¿‡å¤šæŸ¥è¯¢
        selected_keywords = shuffled_keywords[:30]
        
        # å°†å…³é”®è¯åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š10ä¸ªå…³é”®è¯
        batch_size = 10
        keywords_batches = [selected_keywords[i:i + batch_size] for i in range(0, len(selected_keywords), batch_size)]
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° ä»{len(keywords)}ä¸ªå…³é”®è¯ä¸­éšæœºé€‰æ‹©{len(selected_keywords)}ä¸ªï¼Œåˆ†ä¸º{len(keywords_batches)}æ‰¹è¿›è¡ŒæŸ¥è¯¢")
        
        # æ„å»ºNewsAPIè¯·æ±‚URL
        base_url = "https://newsapi.org/v2/everything"
        
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡è·å–çš„æ–‡ç« 
        all_articles = []
        
        # è®¾ç½®æå‰ç»ˆæ­¢æ¡ä»¶ï¼šè·å–åˆ°12ç¯‡æ–‡ç« å°±åœæ­¢
        early_stop_count = 12
        
        # æŒ‰æ‰¹æ¬¡è·å–æ–‡ç« 
        for i, batch_keywords in enumerate(keywords_batches):
            # æ„å»ºæŸ¥è¯¢å…³é”®è¯
            query = " OR ".join(batch_keywords)
            
            # è®¾ç½®è¯·æ±‚å‚æ•°
            params = {
                "apiKey": self.news_api_key,
                "q": query,
                "from": (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                "language": language,
                "sortBy": "relevancy",
                "pageSize": 4  # æ¯æ‰¹æ¬¡è·å–å›ºå®šæ•°é‡çš„æ–‡ç« 
            }
            
            try:
                # å‘é€è¯·æ±‚
                response = requests.get(base_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                # è·å–æ–‡ç« åˆ—è¡¨
                batch_articles = data.get("articles", [])
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° æ‰¹æ¬¡ {i+1}/{len(keywords_batches)}: è·å–åˆ° {len(batch_articles)} ç¯‡æ–‡ç« ")
                
                # å°†è¯¥æ‰¹æ¬¡çš„æ–‡ç« æ·»åŠ åˆ°æ€»æ–‡ç« åˆ—è¡¨ä¸­
                all_articles.extend(batch_articles)
                
                # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œå¯ä»¥æå‰é€€å‡º
                if len(all_articles) >= early_stop_count:
                    print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° å·²è·å–è¶³å¤Ÿå¤šçš„æ–‡ç«  ({len(all_articles)} ç¯‡)ï¼Œåœæ­¢æŸ¥è¯¢")
                    break
                    
            except Exception as e:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è·å–{lang_label}ç»æµæ–°é—»æ‰¹æ¬¡ {i+1} å¤±è´¥: {e}")
        
        # å»é™¤å¯èƒ½çš„é‡å¤æ–‡ç« ï¼ˆåŸºäºURLï¼‰
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            url = article.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° å»é‡åå…± {len(unique_articles)} ç¯‡{lang_label}æ–‡ç« ")
        
        # ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        if unique_articles:
            return self._filter_relevant_articles(unique_articles, max_articles, lang_label)
        else:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° æœªè·å–åˆ°{lang_label}æ–‡ç« ")
            return []
    
    def _filter_relevant_articles(self, articles, max_articles, language_label=""):
        """ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            max_articles (int): æœ€å¤§è¿”å›æ–‡ç« æ•°é‡
            language_label (str): è¯­è¨€æ ‡ç­¾ï¼Œç”¨äºæ—¥å¿—
            
        Returns:
            list: ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
        """
        # å¦‚æœæ–‡ç« æ•°é‡å°‘äºmax_articlesï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ–‡ç« 
        if len(articles) <= max_articles:
            return self._format_articles(articles, language_label)
        
        # æ„å»ºæç¤ºè¯
        article_summaries = []
        for i, article in enumerate(articles[:10]):  # åªå¤„ç†å‰10ç¯‡æ–‡ç« 
            title = article.get("title", "æ— æ ‡é¢˜")
            description = article.get("description", "æ— æè¿°")
            article_summaries.append(f"{i+1}. æ ‡é¢˜: {title}\n   æè¿°: {description}")
        
        article_text = "\n\n".join(article_summaries)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµé‡‘èæ–°é—»ç¼–è¾‘ï¼Œè¯·ä»ä»¥ä¸‹{language_label}ç»æµæ–°é—»ä¸­é€‰æ‹©{max_articles}ç¯‡æœ€é‡è¦ã€æœ€æœ‰å½±å“åŠ›çš„æ–‡ç« ï¼Œå®ƒä»¬åº”è¯¥æ¶µç›–é‡å¤§ç»æµäº‹ä»¶ã€é‡‘èå¸‚åœºåŠ¨æ€æˆ–å…·æœ‰å¹¿æ³›å½±å“çš„äº§ä¸šå˜é©ã€‚
        
æ–°é—»åˆ—è¡¨:
{article_text}

è¯·åªè¿”å›ä½ é€‰æ‹©çš„æ–‡ç« åºå·ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„æ•°å­—ï¼Œä¾‹å¦‚: 1,3,5
"""
        
        # è°ƒç”¨LLM API
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’° ç­›é€‰{language_label}æ–‡ç« ä¸­...")
        response = self.call_llm_api(prompt, temperature=0.3)
        
        try:
            # è§£æå“åº”ï¼Œè·å–é€‰ä¸­çš„æ–‡ç« ç´¢å¼•
            selected_indices = []
            for item in response.replace(" ", "").split(","):
                if item.isdigit() and 1 <= int(item) <= len(articles):
                    selected_indices.append(int(item) - 1)
            
            # æˆªå–è‡³max_articles
            selected_indices = selected_indices[:max_articles]
            
            # è¿”å›é€‰ä¸­çš„æ–‡ç« 
            selected_articles = [articles[i] for i in selected_indices]
            
            return self._format_articles(selected_articles, language_label)
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è§£æLLMå“åº”å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›å‰max_articlesç¯‡æ–‡ç« 
            return self._format_articles(articles[:max_articles], language_label)
    
    def _format_articles(self, articles, language_label=""):
        """æ ¼å¼åŒ–æ–‡ç« ä¸ºç»Ÿä¸€è¾“å‡ºæ ¼å¼
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            language_label (str): è¯­è¨€æ ‡ç­¾
            
        Returns:
            list: æ ¼å¼åŒ–åçš„æ–‡ç« åˆ—è¡¨
        """
        formatted_articles = []
        for article in articles:
            formatted_article = {
                "title": article.get("title", "æ— æ ‡é¢˜"),
                "description": article.get("description", "æ— æè¿°"),
                "url": article.get("url", ""),
                "source": article.get("source", {}).get("name", "æœªçŸ¥æ¥æº"),
                "published_at": article.get("publishedAt", ""),
                "category": "ç»æµ",
                "language": language_label
            }
            formatted_articles.append(formatted_article)
        
        return formatted_articles 