import requests
from datetime import datetime, timedelta
from .base_agent import BaseAgent

class ScienceNewsAgent(BaseAgent):
    """ç§‘å­¦æ–°é—»æ™ºèƒ½ä½“"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘å­¦æ–°é—»æ™ºèƒ½ä½“"""
        super().__init__()
        self.categories = ["science"]
        self.en_keywords = [
            # åŸºç¡€ç§‘å­¦é¢†åŸŸ
            "science", "scientific discovery", "scientific research", "laboratory", "experiment",
            "scientific breakthrough", "scientific innovation", "scientific theory",
            "scientific study", "scientific method", "scientific evidence", "scientists",
            
            # ç”Ÿå‘½ç§‘å­¦
            "biology", "biological", "microbiology", "molecular biology", "genetics", 
            "genomics", "DNA", "RNA", "protein", "cell biology", "evolutionary biology",
            "ecology", "biodiversity", "ecosystem", "species", "conservation biology",
            
            # åŒ»å­¦ä¸å¥åº·
            "medicine", "medical research", "medical breakthrough", "health science", 
            "clinical trial", "drug discovery", "pharmaceutical", "disease", "vaccine",
            "pathogen", "immunology", "epidemiology", "public health", "global health",
            
            # ç”Ÿç‰©æŠ€æœ¯
            "biotech", "biotechnology", "genetic engineering", "CRISPR", "gene editing",
            "synthetic biology", "bioinformatics", "biomaterials", "biofuel", 
            "biomedical engineering", "tissue engineering", "stem cells",
            
            # ç‰©ç†å­¦
            "physics", "astrophysics", "particle physics", "quantum physics", "quantum mechanics",
            "theoretical physics", "nuclear physics", "plasma physics", "condensed matter physics",
            "relativity", "dark matter", "dark energy", "gravitational waves",
            
            # åŒ–å­¦
            "chemistry", "biochemistry", "chemical", "organic chemistry", "inorganic chemistry",
            "analytical chemistry", "physical chemistry", "polymer", "catalyst", "molecule",
            "reaction", "synthesis", "nanomaterials", "electrochemistry",
            
            # åœ°çƒä¸ç¯å¢ƒç§‘å­¦
            "earth science", "geology", "geophysics", "meteorology", "climatology", 
            "oceanography", "atmospheric science", "environmental science", "climate change",
            "global warming", "carbon emissions", "sustainability", "renewable energy",
            
            # å¤ªç©ºç§‘å­¦
            "astronomy", "space science", "cosmology", "planet", "solar system", "galaxy",
            "universe", "NASA", "space exploration", "space telescope", "exoplanet",
            "mars mission", "space station", "satellite", "rocket", "spacecraft"
        ]
        
        self.zh_keywords = [
            # åŸºç¡€ç§‘å­¦é¢†åŸŸ
            "ç§‘å­¦", "ç§‘å­¦å‘ç°", "ç§‘å­¦ç ”ç©¶", "å®éªŒå®¤", "å®éªŒ", "ç§‘å­¦çªç ´", "ç§‘å­¦åˆ›æ–°", 
            "ç§‘å­¦ç†è®º", "ç§‘å­¦ç ”ç©¶", "ç§‘å­¦æ–¹æ³•", "ç§‘å­¦è¯æ®", "ç§‘å­¦å®¶", "é™¢å£«",
            
            # ç”Ÿå‘½ç§‘å­¦
            "ç”Ÿç‰©å­¦", "ç”Ÿç‰©", "å¾®ç”Ÿç‰©å­¦", "åˆ†å­ç”Ÿç‰©å­¦", "é—ä¼ å­¦", "åŸºå› ç»„å­¦", 
            "DNA", "RNA", "è›‹ç™½è´¨", "ç»†èƒç”Ÿç‰©å­¦", "è¿›åŒ–ç”Ÿç‰©å­¦", "ç”Ÿæ€å­¦", 
            "ç”Ÿç‰©å¤šæ ·æ€§", "ç”Ÿæ€ç³»ç»Ÿ", "ç‰©ç§", "ä¿æŠ¤ç”Ÿç‰©å­¦",
            
            # åŒ»å­¦ä¸å¥åº·
            "åŒ»å­¦", "åŒ»å­¦ç ”ç©¶", "åŒ»å­¦çªç ´", "å¥åº·ç§‘å­¦", "ä¸´åºŠè¯•éªŒ", "è¯ç‰©å‘ç°", 
            "åˆ¶è¯", "ç–¾ç—…", "ç–«è‹—", "ç—…åŸä½“", "å…ç–«å­¦", "æµè¡Œç—…å­¦", "å…¬å…±å«ç”Ÿ", "å…¨çƒå¥åº·",
            
            # ç”Ÿç‰©æŠ€æœ¯
            "ç”Ÿç‰©æŠ€æœ¯", "åŸºå› å·¥ç¨‹", "åŸºå› ç¼–è¾‘", "CRISPR", "åˆæˆç”Ÿç‰©å­¦", "ç”Ÿç‰©ä¿¡æ¯å­¦", 
            "ç”Ÿç‰©ææ–™", "ç”Ÿç‰©ç‡ƒæ–™", "ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹", "ç»„ç»‡å·¥ç¨‹", "å¹²ç»†èƒ",
            
            # ç‰©ç†å­¦
            "ç‰©ç†å­¦", "å¤©ä½“ç‰©ç†å­¦", "ç²’å­ç‰©ç†å­¦", "é‡å­ç‰©ç†å­¦", "é‡å­åŠ›å­¦", "ç†è®ºç‰©ç†å­¦", 
            "æ ¸ç‰©ç†å­¦", "ç­‰ç¦»å­ä½“ç‰©ç†å­¦", "å‡èšæ€ç‰©ç†å­¦", "ç›¸å¯¹è®º", "æš—ç‰©è´¨", "æš—èƒ½é‡", "å¼•åŠ›æ³¢",
            
            # åŒ–å­¦
            "åŒ–å­¦", "ç”Ÿç‰©åŒ–å­¦", "æœ‰æœºåŒ–å­¦", "æ— æœºåŒ–å­¦", "åˆ†æåŒ–å­¦", "ç‰©ç†åŒ–å­¦", 
            "èšåˆç‰©", "å‚¬åŒ–å‰‚", "åˆ†å­", "åŒ–å­¦ååº”", "åˆæˆ", "çº³ç±³ææ–™", "ç”µåŒ–å­¦",
            
            # åœ°çƒä¸ç¯å¢ƒç§‘å­¦
            "åœ°çƒç§‘å­¦", "åœ°è´¨å­¦", "åœ°çƒç‰©ç†å­¦", "æ°”è±¡å­¦", "æ°”å€™å­¦", "æµ·æ´‹å­¦", 
            "å¤§æ°”ç§‘å­¦", "ç¯å¢ƒç§‘å­¦", "æ°”å€™å˜åŒ–", "å…¨çƒå˜æš–", "ç¢³æ’æ”¾", "å¯æŒç»­å‘å±•", "å¯å†ç”Ÿèƒ½æº",
            
            # å¤ªç©ºç§‘å­¦
            "å¤©æ–‡å­¦", "å¤ªç©ºç§‘å­¦", "å®‡å®™å­¦", "è¡Œæ˜Ÿ", "å¤ªé˜³ç³»", "é“¶æ²³ç³»", "å®‡å®™", 
            "èˆªå¤©", "å¤ªç©ºæ¢ç´¢", "å¤©æ–‡æœ›è¿œé•œ", "ç³»å¤–è¡Œæ˜Ÿ", "ç«æ˜Ÿä»»åŠ¡", "ç©ºé—´ç«™", "å«æ˜Ÿ", "ç«ç®­", "å®‡å®™é£èˆ¹"
        ]
    
    def collect_news(self, max_articles=5):
        """æ”¶é›†ç§‘å­¦ç›¸å…³æ–°é—»ï¼ŒåŒ…æ‹¬ä¸­è‹±æ–‡å„5æ¡
        
        Args:
            max_articles (int): æ¯ç§è¯­è¨€çš„æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ æ”¶é›†ç§‘å­¦æ–°é—»...")
        
        # è·å–å½“å‰æ—¥æœŸå’Œå‰ä¸€å¤©çš„æ—¥æœŸ
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        from_date = yesterday.strftime('%Y-%m-%d')
        
        # è·å–è‹±æ–‡æ–°é—»
        english_news = self._collect_language_news(self.en_keywords, "en", max_articles)
        
        # è·å–ä¸­æ–‡æ–°é—» - å¤šè·å–ä¸€äº›ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        zh_max = max_articles * 2  # è·å–æ›´å¤šä¸­æ–‡æ–°é—»ä»¥ä¾¿è¿‡æ»¤ç¹ä½“
        chinese_news = self._collect_language_news(self.zh_keywords, "zh", zh_max)
        
        # è¿‡æ»¤ç¹ä½“ä¸­æ–‡æ–°é—»
        simplified_chinese_news = []
        filtered_count = 0
        
        for news in chinese_news:
            # æ£€æŸ¥æ ‡é¢˜å’Œæè¿°æ˜¯å¦åŒ…å«ç¹ä½“ä¸­æ–‡
            title = news.get('title', '')
            desc = news.get('description', '')
            
            if self.is_traditional_chinese(title) or self.is_traditional_chinese(desc):
                filtered_count += 1
                continue
            
            simplified_chinese_news.append(news)
            
            # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿæ•°é‡çš„ç®€ä½“ä¸­æ–‡æ–°é—»ï¼Œå°±åœæ­¢
            if len(simplified_chinese_news) >= max_articles:
                break
        
        if filtered_count > 0:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ å·²è¿‡æ»¤ {filtered_count} æ¡ç¹ä½“ä¸­æ–‡æ–°é—»")
        
        # ç¡®ä¿ä¸è¶…è¿‡max_articlesæ•°é‡
        simplified_chinese_news = simplified_chinese_news[:max_articles]
        
        # åˆå¹¶ç»“æœ
        combined_news = english_news + simplified_chinese_news
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ æ”¶é›†å®Œæˆ: {len(combined_news)} æ¡ (è‹±:{len(english_news)}, ä¸­:{len(simplified_chinese_news)})")
        return combined_news
    
    def _collect_language_news(self, keywords, language, max_articles):
        """æ”¶é›†ç‰¹å®šè¯­è¨€çš„æ–°é—»
        
        Args:
            keywords (list): æœç´¢å…³é”®è¯åˆ—è¡¨
            language (str): è¯­è¨€ä»£ç ï¼Œ'en'ä¸ºè‹±æ–‡ï¼Œ'zh'ä¸ºä¸­æ–‡
            max_articles (int): æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            list: è¯¥è¯­è¨€çš„æ–°é—»æ–‡ç« åˆ—è¡¨
        """
        lang_label = "è‹±æ–‡" if language == "en" else "ä¸­æ–‡"
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ è·å–{lang_label}ç§‘å­¦æ–°é—»...")
        
        # å¯¼å…¥éšæœºæ¨¡å—
        import random
        
        # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        # åªé€‰å–å‰30ä¸ªå…³é”®è¯ï¼Œé¿å…è¿‡å¤šæŸ¥è¯¢
        selected_keywords = shuffled_keywords[:30]
        
        # å°†å…³é”®è¯åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š10ä¸ªå…³é”®è¯
        batch_size = 10
        keywords_batches = [selected_keywords[i:i + batch_size] for i in range(0, len(selected_keywords), batch_size)]
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ ä»{len(keywords)}ä¸ªå…³é”®è¯ä¸­éšæœºé€‰æ‹©{len(selected_keywords)}ä¸ªï¼Œåˆ†ä¸º{len(keywords_batches)}æ‰¹è¿›è¡ŒæŸ¥è¯¢")
        
        # æ„å»ºNewsAPIè¯·æ±‚URL
        base_url = "https://newsapi.org/v2/everything"
        
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡è·å–çš„æ–‡ç« 
        all_articles = []
        
        # è®¾ç½®æå‰ç»ˆæ­¢æ¡ä»¶ï¼šè·å–åˆ°12ç¯‡æ–‡ç« å°±åœæ­¢
        early_stop_count = 12
        
        # æŒ‰æ‰¹æ¬¡è·å–æ–‡ç« 
        for i, batch_keywords in enumerate(keywords_batches):
            # æ„å»ºæŸ¥è¯¢å…³é”®è¯
            query = " OR ".join(batch_keywords)
            
            # è®¾ç½®è¯·æ±‚å‚æ•°
            params = {
                "apiKey": self.news_api_key,
                "q": query,
                "from": (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                "language": language,
                "sortBy": "relevancy",
                "pageSize": 4  # æ¯æ‰¹æ¬¡è·å–å›ºå®šæ•°é‡çš„æ–‡ç« 
            }
            
            try:
                # å‘é€è¯·æ±‚
                response = requests.get(base_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                # è·å–æ–‡ç« åˆ—è¡¨
                batch_articles = data.get("articles", [])
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ æ‰¹æ¬¡ {i+1}/{len(keywords_batches)}: è·å–åˆ° {len(batch_articles)} ç¯‡æ–‡ç« ")
                
                # å°†è¯¥æ‰¹æ¬¡çš„æ–‡ç« æ·»åŠ åˆ°æ€»æ–‡ç« åˆ—è¡¨ä¸­
                all_articles.extend(batch_articles)
                
                # å¦‚æœå·²ç»è·å–è¶³å¤Ÿå¤šçš„æ–‡ç« ï¼Œå¯ä»¥æå‰é€€å‡º
                if len(all_articles) >= early_stop_count:
                    print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ å·²è·å–è¶³å¤Ÿå¤šçš„æ–‡ç«  ({len(all_articles)} ç¯‡)ï¼Œåœæ­¢æŸ¥è¯¢")
                    break
                    
            except Exception as e:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è·å–{lang_label}ç§‘å­¦æ–°é—»æ‰¹æ¬¡ {i+1} å¤±è´¥: {e}")
        
        # å»é™¤å¯èƒ½çš„é‡å¤æ–‡ç« ï¼ˆåŸºäºURLï¼‰
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            url = article.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ å»é‡åå…± {len(unique_articles)} ç¯‡{lang_label}æ–‡ç« ")
        
        # ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        if unique_articles:
            return self._filter_relevant_articles(unique_articles, max_articles, lang_label)
        else:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ æœªè·å–åˆ°{lang_label}æ–‡ç« ")
            return []
    
    def _filter_relevant_articles(self, articles, max_articles, language_label=""):
        """ä½¿ç”¨LLMç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« 
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            max_articles (int): æœ€å¤§è¿”å›æ–‡ç« æ•°é‡
            language_label (str): è¯­è¨€æ ‡ç­¾ï¼Œç”¨äºæ—¥å¿—
            
        Returns:
            list: ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
        """
        # å¦‚æœæ–‡ç« æ•°é‡å°‘äºmax_articlesï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ–‡ç« 
        if len(articles) <= max_articles:
            return self._format_articles(articles, language_label)
        
        # æ„å»ºæç¤ºè¯
        article_summaries = []
        for i, article in enumerate(articles[:10]):  # åªå¤„ç†å‰10ç¯‡æ–‡ç« 
            title = article.get("title", "æ— æ ‡é¢˜")
            description = article.get("description", "æ— æè¿°")
            article_summaries.append(f"{i+1}. æ ‡é¢˜: {title}\n   æè¿°: {description}")
        
        article_text = "\n\n".join(article_summaries)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦æ–°é—»ç¼–è¾‘ï¼Œè¯·ä»ä»¥ä¸‹{language_label}ç§‘å­¦æ–°é—»ä¸­é€‰æ‹©{max_articles}ç¯‡æœ€é‡è¦ã€æœ€æœ‰å½±å“åŠ›çš„æ–‡ç« ï¼Œå®ƒä»¬åº”è¯¥æ¶µç›–é‡è¦ç§‘å­¦å‘ç°ã€ç ”ç©¶çªç ´æˆ–å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼çš„ç§‘å­¦è¿›å±•ã€‚
        
æ–°é—»åˆ—è¡¨:
{article_text}

è¯·åªè¿”å›ä½ é€‰æ‹©çš„æ–‡ç« åºå·ï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„æ•°å­—ï¼Œä¾‹å¦‚: 1,3,5
"""
        
        # è°ƒç”¨LLM API
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”¬ ç­›é€‰{language_label}æ–‡ç« ä¸­...")
        response = self.call_llm_api(prompt, temperature=0.3)
        
        try:
            # è§£æå“åº”ï¼Œè·å–é€‰ä¸­çš„æ–‡ç« ç´¢å¼•
            selected_indices = []
            for item in response.replace(" ", "").split(","):
                if item.isdigit() and 1 <= int(item) <= len(articles):
                    selected_indices.append(int(item) - 1)
            
            # æˆªå–è‡³max_articles
            selected_indices = selected_indices[:max_articles]
            
            # è¿”å›é€‰ä¸­çš„æ–‡ç« 
            selected_articles = [articles[i] for i in selected_indices]
            
            return self._format_articles(selected_articles, language_label)
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ è§£æLLMå“åº”å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›å‰max_articlesç¯‡æ–‡ç« 
            return self._format_articles(articles[:max_articles], language_label)
    
    def _format_articles(self, articles, language_label=""):
        """æ ¼å¼åŒ–æ–‡ç« ä¸ºç»Ÿä¸€è¾“å‡ºæ ¼å¼
        
        Args:
            articles (list): åŸå§‹æ–‡ç« åˆ—è¡¨
            language_label (str): è¯­è¨€æ ‡ç­¾
            
        Returns:
            list: æ ¼å¼åŒ–åçš„æ–‡ç« åˆ—è¡¨
        """
        formatted_articles = []
        for article in articles:
            formatted_article = {
                "title": article.get("title", "æ— æ ‡é¢˜"),
                "description": article.get("description", "æ— æè¿°"),
                "url": article.get("url", ""),
                "source": article.get("source", {}).get("name", "æœªçŸ¥æ¥æº"),
                "published_at": article.get("publishedAt", ""),
                "category": "ç§‘å­¦",
                "language": language_label
            }
            formatted_articles.append(formatted_article)
        
        return formatted_articles 